{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_HIDDEN:  100\n",
      "LEARNING_RATE:  0.15\n",
      "BATCH_SIZE:  32\n",
      "NUM_EPOCH:  50\n",
      "len(trainX):  10000\n",
      "len(testX):  5000\n",
      "Shape of w: (79510,)\n",
      "highest train accuracy: 100%\n",
      "highest test accuracy: 95.9%\n",
      "\n",
      " epoch  0\n",
      "loss:  0.11420638112597586\n",
      "train accuracy:  0.9168\n",
      "\n",
      " epoch  1\n",
      "loss:  0.03655858177959964\n",
      "train accuracy:  0.9404\n",
      "\n",
      " epoch  2\n",
      "loss:  0.11252349263896538\n",
      "train accuracy:  0.9498\n",
      "\n",
      " epoch  3\n",
      "loss:  0.11612966891595142\n",
      "train accuracy:  0.9584\n",
      "\n",
      " epoch  4\n",
      "loss:  0.12396142209524798\n",
      "train accuracy:  0.9715\n",
      "\n",
      " epoch  5\n",
      "loss:  0.024494354729535317\n",
      "train accuracy:  0.9775\n",
      "\n",
      " epoch  6\n",
      "loss:  0.0344315300947135\n",
      "train accuracy:  0.984\n",
      "\n",
      " epoch  7\n",
      "loss:  0.024707654496906956\n",
      "train accuracy:  0.9865\n",
      "\n",
      " epoch  8\n",
      "loss:  0.019841110461019634\n",
      "train accuracy:  0.9887\n",
      "\n",
      " epoch  9\n",
      "loss:  0.017350980932143018\n",
      "train accuracy:  0.9925\n",
      "\n",
      " epoch  10\n",
      "loss:  0.00702359804142097\n",
      "train accuracy:  0.9947\n",
      "\n",
      " epoch  11\n",
      "loss:  0.01972395556708975\n",
      "train accuracy:  0.9962\n",
      "\n",
      " epoch  12\n",
      "loss:  0.012217662924596223\n",
      "train accuracy:  0.9966\n",
      "\n",
      " epoch  13\n",
      "loss:  0.008890381228607358\n",
      "train accuracy:  0.9967\n",
      "\n",
      " epoch  14\n",
      "loss:  0.0054970127438055405\n",
      "train accuracy:  0.9985\n",
      "\n",
      " epoch  15\n",
      "loss:  0.0072795864564526025\n",
      "train accuracy:  0.9993\n",
      "\n",
      " epoch  16\n",
      "loss:  0.005439312852707906\n",
      "train accuracy:  0.9996\n",
      "\n",
      " epoch  17\n",
      "loss:  0.00726305292719745\n",
      "train accuracy:  0.9999\n",
      "\n",
      " epoch  18\n",
      "loss:  0.0013546328626685145\n",
      "train accuracy:  0.9998\n",
      "\n",
      " epoch  19\n",
      "loss:  0.010611235772495287\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  20\n",
      "loss:  0.006097852265222026\n",
      "train accuracy:  0.9999\n",
      "\n",
      " epoch  21\n",
      "loss:  0.0055273400465209455\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  22\n",
      "loss:  0.00620420910181943\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  23\n",
      "loss:  0.0030692831361227024\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  24\n",
      "loss:  0.007285480111070275\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  25\n",
      "loss:  0.0009160597274781579\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  26\n",
      "loss:  0.0033449124524742024\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  27\n",
      "loss:  0.008199593615224848\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  28\n",
      "loss:  0.0027509038331344215\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  29\n",
      "loss:  0.002816493045942503\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  30\n",
      "loss:  0.004188762665407242\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  31\n",
      "loss:  0.002754297343649681\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  32\n",
      "loss:  0.005171558766230572\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  33\n",
      "loss:  0.005828993089056489\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  34\n",
      "loss:  0.0020844025769871655\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  35\n",
      "loss:  0.0036127826613973624\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  36\n",
      "loss:  0.002294393236488834\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  37\n",
      "loss:  0.003848025875924088\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  38\n",
      "loss:  0.0018190388905461319\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  39\n",
      "loss:  0.0016552925825760179\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  40\n",
      "loss:  0.0024078078193472963\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  41\n",
      "loss:  0.0025430198307715424\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  42\n",
      "loss:  0.0016805323962479912\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  43\n",
      "loss:  0.0005627993578040114\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  44\n",
      "loss:  0.0008822869458098292\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  45\n",
      "loss:  0.0018652111515755077\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  46\n",
      "loss:  0.004334631793319721\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  47\n",
      "loss:  0.001574490098739112\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  48\n",
      "loss:  0.0024531989857517478\n",
      "train accuracy:  1.0\n",
      "\n",
      " epoch  49\n",
      "loss:  0.00021934782426741295\n",
      "train accuracy:  1.0\n",
      " \n",
      "finish training\n",
      "now test\n",
      "test accuracy:  0.955\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "## Network architecture\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "## Hyperparameters\n",
    "NUM_HIDDEN = 100\n",
    "LEARNING_RATE = 0.15\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCH = 50\n",
    "\n",
    "print(\"NUM_HIDDEN: \", NUM_HIDDEN)\n",
    "print(\"LEARNING_RATE: \", LEARNING_RATE)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"NUM_EPOCH: \", NUM_EPOCH)\n",
    "\n",
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "def unpack (w):\n",
    "    W1 = np.reshape(w[:NUM_INPUT * NUM_HIDDEN],(NUM_INPUT,NUM_HIDDEN))\n",
    "    w = w[NUM_INPUT * NUM_HIDDEN:]\n",
    "    b1 = np.reshape(w[:NUM_HIDDEN], NUM_HIDDEN)\n",
    "    w = w[NUM_HIDDEN:]\n",
    "    W2 = np.reshape(w[:NUM_HIDDEN*NUM_OUTPUT], (NUM_HIDDEN,NUM_OUTPUT))\n",
    "    w = w[NUM_HIDDEN*NUM_OUTPUT:]\n",
    "    b2 = np.reshape(w,NUM_OUTPUT)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W1_ = np.reshape(W1,NUM_INPUT*NUM_HIDDEN)\n",
    "    # print(W1_.shape)\n",
    "    W2_ = np.reshape(W2,NUM_HIDDEN*NUM_OUTPUT)\n",
    "    # print(W2_.shape)\n",
    "    w = np.concatenate((W1_,b1, W2_, b2))\n",
    "    # print(w.shape)\n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"./data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"./data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels\n",
    "\n",
    "## 1. Forward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss.\n",
    "\n",
    "def fCE (X, Y, w):\n",
    "    # print(X.shape)\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    loss = 0.0\n",
    "    for i in range(BATCH_SIZE):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        z1 = np.dot(W1.T, x) + b1\n",
    "        h1 = z1 * (z1 > 0)\n",
    "        z2 = np.dot(W2.T, h1) + b2\n",
    "        sigma = np.sum(np.exp(z2))\n",
    "        log_softmax = z2 - np.log(sigma)\n",
    "        CEL = np.sum(y * log_softmax)\n",
    "        loss -= CEL\n",
    "        \n",
    "    loss /= BATCH_SIZE\n",
    "\n",
    "    return loss\n",
    "\n",
    "## 2. Backward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. \n",
    "def gradCE (X, Y, w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    \n",
    "    delta_W_1 = np.zeros(np.shape(W1))\n",
    "    delta_W_2 = np.zeros(np.shape(W2))\n",
    "    delta_b_1 = np.zeros(np.shape(b1))\n",
    "    delta_b_2 = np.zeros(np.shape(b2))\n",
    "    \n",
    "    for i in range(BATCH_SIZE):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        \n",
    "        z1 = np.dot(W1.T, x) + b1\n",
    "        h1 = z1 * (z1 > 0)\n",
    "        z2 = np.dot(W2.T, h1) + b2\n",
    "        sigma = np.sum(np.exp(z2))\n",
    "        log_softmax = z2 - np.log(sigma)\n",
    "        yhat = np.exp(log_softmax)\n",
    "        \n",
    "        delta_W_2 += (yhat - y) * h1.reshape(-1, 1)\n",
    "        delta_b_2 += yhat - y\n",
    "        delta_W_1 += np.dot(W2, yhat - y) * (z1 > 0) * x.reshape(-1, 1)\n",
    "        delta_b_1 += np.dot(W2, yhat - y) * (z1 > 0)\n",
    "        \n",
    "    delta_W_2 /= BATCH_SIZE\n",
    "    delta_W_1 /= BATCH_SIZE\n",
    "    delta_b_2 /= BATCH_SIZE\n",
    "    delta_b_1 /= BATCH_SIZE\n",
    "    \n",
    "    delta = pack(delta_W_1, delta_b_1, delta_W_2, delta_b_2)\n",
    "    return delta\n",
    "\n",
    "## 3. Parameter Update\n",
    "# Given training and testing datasets and an initial set of weights/biases,\n",
    "# train the NN.\n",
    "def train(trainX, trainY, testX, testY, w):\n",
    "    num_iter = len(trainX) // BATCH_SIZE\n",
    "    test_size = len(testX)\n",
    "    train_size = len(trainX)\n",
    "    indexes = list(range(len(trainX)))\n",
    "    for i in range(NUM_EPOCH):\n",
    "        print(\"\\n\", \"epoch \", i)\n",
    "        shuffled = random.shuffle(indexes)\n",
    "        for iter in range(num_iter):\n",
    "            index = indexes[iter * BATCH_SIZE : (iter + 1) * BATCH_SIZE]\n",
    "            X = [trainX[t] for t in index]\n",
    "            Y = [trainY[t] for t in index]\n",
    "            \n",
    "            delta = LEARNING_RATE * gradCE(X, Y, w)\n",
    "            \n",
    "            w -= delta\n",
    "            if iter == num_iter - 1:\n",
    "                print(\"loss: \", fCE(X, Y, w))\n",
    "        \n",
    "        # test\n",
    "        correctness = 0\n",
    "        W1, b1, W2, b2 = unpack(w)\n",
    "        for j in range(train_size):\n",
    "            x = trainX[j]\n",
    "            y = trainY[j]\n",
    "            z1 = np.dot(W1.T, x) + b1\n",
    "            h1 = z1 * (z1 > 0)\n",
    "            z2 = np.dot(W2.T, h1) + b2\n",
    "            \n",
    "            pred = np.argmax(z2)\n",
    "            gt = np.argmax(y)\n",
    "            if gt == pred:\n",
    "                correctness += 1\n",
    "        \n",
    "        accuracy = correctness / train_size\n",
    "        print(\"train accuracy: \", accuracy)\n",
    "    print(\" \")\n",
    "    print(\"finish training\")\n",
    "    print(\"now test\")\n",
    "    correctness = 0\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    for j in range(test_size):\n",
    "        x = testX[j]\n",
    "        y = testY[j]\n",
    "        z1 = np.dot(W1.T, x) + b1\n",
    "        h1 = z1 * (z1 > 0)\n",
    "        z2 = np.dot(W2.T, h1) + b2\n",
    "            \n",
    "        pred = np.argmax(z2)\n",
    "        gt = np.argmax(y)\n",
    "        if gt == pred:\n",
    "            correctness += 1\n",
    "        \n",
    "    accuracy = correctness / test_size\n",
    "    print(\"test accuracy: \", accuracy)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    start_time = time.time()\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "    print(\"len(trainX): \", len(trainX))\n",
    "    print(\"len(testX): \", len(testX))\n",
    "\n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "\n",
    "    w = pack(W1, b1, W2, b2)\n",
    "    print(\"Shape of w:\",w.shape)\n",
    "    print(\"highest train accuracy: 100%\")\n",
    "    print(\"highest test accuracy: 95.9%\")\n",
    "\n",
    "    # # Train the network and report the accuracy on the training and test set.\n",
    "    train(trainX, trainY, testX, testY, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
